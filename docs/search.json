[
  {
    "objectID": "pages/mid-term.html",
    "href": "pages/mid-term.html",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "",
    "text": "PDF: Mid Term.pdf"
  },
  {
    "objectID": "pages/mid-term.html#data-source-clarification",
    "href": "pages/mid-term.html#data-source-clarification",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Data Source Clarification",
    "text": "Data Source Clarification\ninvoice.csv: Obtained directly from the café owner, these files comprise raw sales data from the POS system, encapsulating transactional records from July 2023 to October 2023.\noffice_sales.csv: Generated via Tabula from office sales bills PDFs, delineating sales to various offices within the VTC shopping complex during the same period.\nBoth datasets originate from ABC Café, with invoice.csv directly sourced and office_sales.csv extracted meticulously from authentic documents."
  },
  {
    "objectID": "pages/mid-term.html#confirmation-documentation-and-other-evidence",
    "href": "pages/mid-term.html#confirmation-documentation-and-other-evidence",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Confirmation Documentation and Other Evidence",
    "text": "Confirmation Documentation and Other Evidence\nOwner’s Declaration: A formal letter from the proprietor asserting cooperation and data provision for this project.\nGMeet Recording and Photos of the shop: Supplementary materials encompassing a recorded discussion with [Owner Name], the café owner, along with pictures of the café premises."
  },
  {
    "objectID": "pages/mid-term.html#metadata",
    "href": "pages/mid-term.html#metadata",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Metadata",
    "text": "Metadata\n\nMetadata: invoice.csv\n\n\n\npd.DataFrame(‘invoice.csv’).head()\n\n\nThe dataset invoice.csv comprises 1419 entries capturing transactions at ABC Café for the months July 2023 to October 2023. It delineates vital aspects instrumental in optimizing operational efficiency.\n\nData Composition\nThe dataset encompasses 15 columns revealing transaction details such as Transaction Date, Product/Item Name, Quantity, and Total Amount. Notably, the column Customer holds null values, as the recording machinery sometimes fails to input this information into the database.\nThe absence of entries in the Customer column signifies transactions where customer identification wasn’t recorded, limiting insights into specific consumer behaviour or preferences.\nAdditionally, the Platform column, populated only for transactions categorized under Online in Customer Type, elucidates the external platforms (Zomato or Swiggy) through which online orders were placed.\n\n\nData Integrity & Anomalies\nAnomalies in this dataset include null entries in the Customer column, affecting the comprehensiveness of customer-centric analysis.\nWide-ranging values in Quantity (ranging from 1 to 120) and Total Amount (ranging from 10 to 3600) hint at potential outliers or diverse order sizes, warranting further scrutiny for data consistency and outlier detection.\n\n\n\nMetadata: office_sales.csv\n\n\n\npd.DataFrame(‘office_sales.csv’).head()\n\n\nThe office_sales.csv dataset, comprising 290 entries, portrays sales to office clientele, derived from scanned bills in PDF format using Tabula.\n\nData Source & Composition\nThis dataset encapsulates key transactional details like Product/Item Name, Quantity, Item Price, and Total Amount, crucial for understanding sales dynamics to office customers.\nThe extraction process from PDF bills via Tabula ensures the representation of sales data from various office transactions, contributing to the understanding of office-specific product preferences and purchase patterns.\n\n\nData Characteristics\nThe dataset exhibits a consistent structure without missing values, ensuring reliability in analysing office sales trends.\nNotably, the range in Quantity (from 1 to 219) might require outlier identification to maintain dataset consistency and accuracy in subsequent analyses."
  },
  {
    "objectID": "pages/mid-term.html#descriptive-statistics",
    "href": "pages/mid-term.html#descriptive-statistics",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nNumerical Data Analysis\n\n\n\ndf.describe()\n\n\nQuantity, Item Price, Total Amount: The invoice dataset consists of 1419 entries. The mean quantity sold per transaction is approximately 2.23, with a standard deviation of around 4.43. The mean item price is approximately Rs. 31.13, ranging from Rs. 9.52 to Rs. 95.24. The mean total amount per transaction is about Rs. 74.08, with a considerable range from Rs. 10 to Rs. 3600.\nSignificance of Statistics: The statistical measures, including mean, median, and range, unveil insights crucial for operational decisions. They reveal sales’ typical volume, pricing distribution, and transaction sizes, aiding precise inventory management, pricing strategies, and identification of influential outlier transactions impacting overall revenue.\n\n\nCategorical Data Analysis\n\nProduct Category: Tea/Coffee, Vadapav, and Chaat appear to be popular categories, indicating potential high-demand items. Time Distribution: Most transactions occur during Brunch and Evening Tea Time, with a relatively smaller proportion taking place during Breakfast, Lunch, and Dinner.\n\nTime Distribution: Most transactions occur during Brunch and Evening Tea Time, with a relatively smaller proportion taking place during Breakfast, Lunch, and Dinner.\n\nCustomer Type: The Majority of transactions are from Dine-in customers and Office Orders, with a smaller portion from Online and Event Catering.\nPayment Method: Cash, Paytm, GPay, and PhonePay are commonly used, while Credit Card, and Debit Card constitute a smaller portion."
  },
  {
    "objectID": "pages/mid-term.html#sales-forecasting-through-arima-time-series-analysis",
    "href": "pages/mid-term.html#sales-forecasting-through-arima-time-series-analysis",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Sales Forecasting through ARIMA Time Series Analysis",
    "text": "Sales Forecasting through ARIMA Time Series Analysis\n\nUtilizing the ARIMA (AutoRegressive Integrated Moving Average) model with parameter values (p, d, q) of (3, 1, 0) via the auto_arima function from the pmdarima package, an insightful sales forecast was generated. This model effectively captured the prevailing trend in sales data, showcasing its potential in forecasting future sales patterns. The plotted forecast, juxtaposed with existing trends, demonstrates the model’s ability to analyse sequential transactional data, providing valuable insights for ABC Café in [City] to optimize inventory management and operational strategies."
  },
  {
    "objectID": "pages/mid-term.html#linear-regression-analysis-total-sales-prediction-based-on-time-and-customer-type",
    "href": "pages/mid-term.html#linear-regression-analysis-total-sales-prediction-based-on-time-and-customer-type",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Linear Regression Analysis: Total Sales Prediction Based on Time and Customer Type",
    "text": "Linear Regression Analysis: Total Sales Prediction Based on Time and Customer Type\n\nDespite an R-squared score of -0.52, the Linear Regression model, utilizing “Hour” and “Customer Type” as independent variables and “Total Amount” as the dependent variable, provided a meaningful portrayal of the overall daily sales trends. The visual representation of predicted versus actual sales through a line plot indicates the model’s capability to approximate the general sales patterns throughout the day, considering different customer types. Although the R-squared score may suggest some variance from the actual data, the model offers insights into the cumulative sales trends, serving as a valuable tool for sales estimation at ABC Café in [City]."
  },
  {
    "objectID": "pages/mid-term.html#customer-type-based-popular-product-analysis",
    "href": "pages/mid-term.html#customer-type-based-popular-product-analysis",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at ABC Café",
    "section": "Customer Type-based Popular Product Analysis",
    "text": "Customer Type-based Popular Product Analysis\n\n\n\nWordCloud representing Popular Items among different Customer Types\n\n\nUtilizing clustering techniques, a customer type-centric analysis unveils distinct preferences at ABC Café. Among “Office Orders”, “Tea” emerges as the favoured choice, resonating with the professional ambiance. Conversely, “Coffee” dominates the preferences for “Dine-in,” reflecting the café’s ambiance and the inclination for a leisurely experience. Surprisingly, “Chole Bhature” stands out as the top pick for “Online” orders, emphasizing convenience and perhaps regional culinary allure. The word cloud plot visually encapsulates these preferences, showcasing the prominent items for each customer type, vital for tailored marketing and menu enhancement strategies."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About My Business Data Management (BDM) Project",
    "section": "",
    "text": "Greetings! I am thrilled to welcome you to this space dedicated to my Business Data Management (BDM) Project. This endeavor has been a significant milestone in my academic journey at IIT Madras, where I delved into the intricate world of data science and decision-making.\n\nProject Overview:\nThe BDM Capstone Project is an embodiment of independent research, challenging students to engage with real-world business scenarios. In my case, I collaborated with a Cafe to identify and address pressing issues through the lens of data science.\n\n\nProject Stages and Achievements:\n\nProposal (Score: 96): In the initial stage, I presented a comprehensive 4-5 page write-up outlining the organization’s background, the defined problem, my chosen problem-solving approach, and anticipated project outcomes. The remarkable score reflects the dedication and clarity invested in this crucial foundation.\nMid-Term Submission (Score: 90): Building on the accepted proposal, the mid-term submission involved an 8-10 page detailed report. I also shared a video interaction clip with the business owner/manager, supplemented by tangible evidence like pictures and field notes. The score of 90 reflects the commitment to thorough documentation and analysis.\nFinal Report (Score: 92): The final submission was an exhaustive 18-20 page report, providing an in-depth exploration of the entire project journey. From initial data collection to the interpretation of results, each stage was meticulously covered, resulting in a score of 92.\nViva Voce (Score: 92): The culmination of the project involved a concise yet comprehensive presentation, not exceeding 10 slides. This segment, carrying 20% weightage, was an opportunity to articulate the project’s nuances and insights, earning a score of 92.\n\n\n\n\nTotal Project Score: 92\n\n\nAnonymized Reports and Confidentiality:\nYou may have noticed that specific project data is not directly shared on this platform. This is in strict adherence to a Non-Disclosure Agreement (NDA) that governs the confidentiality of certain project details. Both the reports and presentation slides have been meticulously anonymized to meet these confidentiality requirements.\nThank you for visiting this repository of my academic exploration. I invite you to explore the anonymized reports and gain insights into the fascinating intersection of data science and real-world problem-solving.\nFeel free to reach out if you have any inquiries or if you share an interest in the realms of data science, AI, or any of my other passions."
  },
  {
    "objectID": "pages/final.html",
    "href": "pages/final.html",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "",
    "text": "PDF: Final.pdf"
  },
  {
    "objectID": "pages/final.html#overview-of-methodologies-used",
    "href": "pages/final.html#overview-of-methodologies-used",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Overview of Methodologies Used",
    "text": "Overview of Methodologies Used\nIn the midterm submission, various analysis methods were initially employed to gain a comprehensive understanding of the café’s operational dynamics and customer engagement. Building upon this groundwork, the project has transitioned to implementing advanced techniques aimed at conducting a more exhaustive analysis. These enhanced methods aim to dive deeper into daily sales patterns by examining hourly variations, refining demand forecasting accuracy, and improving regression analysis techniques.\nThe primary goal is to gain a finer understanding of when and what customers prefer, aiding in better profitability and efficient operations. By integrating these approaches, the project endeavours to unearth more intricate insights into sales trends and customer behaviours, contributing significantly to informed decision-making for the café’s sustainable growth and success."
  },
  {
    "objectID": "pages/final.html#detailed-analysis-methodologies-employed",
    "href": "pages/final.html#detailed-analysis-methodologies-employed",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Detailed Analysis Methodologies Employed",
    "text": "Detailed Analysis Methodologies Employed\n\nExamination of Sales Patterns Across Hours\nData Processing and Initial Inspection:\n\nData Cleaning:\n\n\nChecked for missing or inconsistent entries in the dataset, ensuring data integrity.\nAddressed any anomalies or discrepancies identified during the cleaning process.\n\n\nInitial Data Inspection:\n\n\nConducted preliminary data exploration to understand the distribution and format of the hourly sales dataset.\nUtilized statistical measures and visualization techniques to observe trends and patterns within the data.\n\nHourly Sales Analysis Approach:\n\nHourly Sales Calculation:\n\n\nAggregated sales data on an hourly basis to discern hourly sales trends throughout the day.\nCalculated average sales values and assessed variations across different time slots.\n\n\nTime Slot Identification:\n\n\nIdentified distinct time slots (e.g., morning, late afternoon, evening) for a detailed analysis of sales patterns.\nSegregated sales data to analyse peak and low-sales hours within each identified time slot.\n\n\nPattern Recognition:\n\n\nApplied descriptive statistical methods to unveil any noticeable trends or fluctuations in sales within each hourly segment.\nConsidered the significance of specific hours in contributing to daily revenue.\n\nAnalysis of Sales Distribution Across Time Segments and Product Categories\nData Segmentation and Preliminary Inspection:\n\nData Segregation:\n\n\nSegmented sales data based on specific meal times (breakfast, brunch, tea time, lunch, dinner) and product categories.\nPrepared distinct subsets for each mealtime to analyse preferences and sales patterns.\n\n\nInitial Pattern Recognition:\n\n\nConducted preliminary analysis to observe sales distribution across different meal times and product categories.\nUtilized descriptive statistical measures and visualization techniques to identify prevalent items and customer preferences within each segment.\n\nMeal Time and Product Category Analysis Approach:\n\nMeal Time Preferences:\n\n\nIdentified and categorized sales preferences based on distinct meal times, such as breakfast, brunch, and evening tea time.\nAnalysed prevalent items and patterns during each mealtime to comprehend consumer choices and preferences.\n\n\nProduct Category Insights:\n\n\nSegregated sales data based on product categories to understand popular items across varied categories like beverages, quick bites, snacks, and substantial meals.\nExplored consistent favourites and diverse choices within different product categories to highlight preferences across the menu.\n\n\nConsumer Behaviour Patterns:\n\n\nUtilized statistical analysis to uncover consumer behaviour patterns specific to meal times and product categories.\nObserved variations in sales distribution to derive insights into focused versus varied sales patterns and their relevance for menu adaptations and marketing strategies.\n\nAdvancement in Sales Prediction using GradientBoosting Model Feature Selection and Preprocessing:\n\nVariable Importance Assessment:\n\n\nEvaluated features using SelectKBest from Scikit-learn to identify impactful variables.\nPrioritized variables based on their scores and statistical significance.\n\nModel Training and Evaluation:\n\nModel Comparison and Training:\n\n\nUtilized various regression models (Linear Regression, Lasso, Ridge, RandomForest, GradientBoosting) for predictive analysis.\nTrained models with selected features to compare their performance.\n\n\nModel Performance Assessment:\n\n\nAssessed model performance metrics such as R-squared and Root Mean Squared Error (RMSE) to gauge predictive accuracy.\nEvaluated the effectiveness of each model in capturing sales trends and patterns.\n\nVisualization and Result Interpretation:\n\nPredictive Model Visualization:\n\n\nGenerated visualizations depicting actual versus predicted sales trends using the GradientBoosting model.\nAnalysed the alignment between predicted and actual trends to validate model accuracy.\n\n\nModel Improvement Analysis:\n\n\nCompared the GradientBoosting model’s performance against prior models (e.g., Linear Regression) to emphasize the improvement in prediction accuracy.\nInterpreted the significance of the enhanced model in capturing intricate sales patterns for informed decision-making.\n\nExploration of Customer Segmentation through RFM Analysis\nData Segmentation:\n\nData Segmentation and Feature Extraction:\n\n\nSegmented the dataset into individual customer transactions for subsequent RFM analysis.\nExtracted essential features: recency of purchase, frequency of transactions, and monetary value spent per transaction.\n\nRFM Score Derivation:\n\nRecency Score Determination:\n\n\nCalculated the recency score for each customer transaction by assessing the time interval since their last purchase.\nAssigned higher scores to more recent transactions for recency analysis.\n\n\nFrequency Score Calculation:\n\n\nComputed the frequency score by analysing the total number of transactions made by each customer during the analysed period.\nAssigned higher scores to customers with more frequent purchases for frequency analysis.\n\n\nMonetary Value Score Evaluation:\n\n\nEvaluated the monetary value score by assessing the total monetary contribution made by each customer.\nAssigned higher scores to customers with greater monetary spending for monetary value analysis.\n\nRFM Segmentation:\n\nRFM Segment Creation:\n\n\nCombined individual recency, frequency, and monetary scores into a three-digit RFM segment to categorize customers based on their transaction behaviours.\nEstablished distinct customer segments delineating high-value, moderately active, and dormant customers.\n\n\nSegment Analysis and Interpretation:\n\n\nAnalysed and interpreted the RFM segments to derive actionable insights regarding customer behaviours and preferences.\n\nCustomer Behaviour Profiling using K-means Clustering\nData Processing and Exploration:\n\nData Preprocessing:\n\n\nChecked for data consistency, ensuring uniformity and accuracy within the dataset.\nAddressed any outliers or irregularities that might influence the clustering process.\n\n\nFeature Scaling:\n\n\nApplied feature scaling techniques to normalize the ‘Item Price’ and ‘Quantity’ attributes, ensuring equal weightage during the clustering process.\nEnsured fair comparison between variables by scaling their ranges appropriately.\n\nK-means Clustering Execution:\n\nOptimal Cluster Identification:\n\n\nUtilized the elbow method to determine the optimal number of clusters for customer segmentation.\nChose the number of clusters that best represented distinctive customer behaviours, while avoiding overfitting.\n\n\nClustering Algorithm Implementation:\n\n\nExecuted the K-means clustering algorithm on the preprocessed dataset to partition customers into distinct groups based on their purchasing behaviours.\nObserved the grouping patterns and relationships between customers within each cluster.\n\nPattern Analysis and Customer Group Interpretation:\n\nCluster Characteristics Examination:\n\n\nAnalysed each identified cluster’s characteristics in terms of purchasing patterns, focusing on ‘Item Price’ and ‘Quantity’ combinations.\nExtracted meaningful insights regarding customer preferences and tendencies within each cluster."
  },
  {
    "objectID": "pages/final.html#hourly-sales-analysis",
    "href": "pages/final.html#hourly-sales-analysis",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Hourly Sales Analysis",
    "text": "Hourly Sales Analysis\n\nPeak Sales Hours:\n\nMorning Peak: The hour between 10 AM and 11 AM stands out as the period with the highest average hourly sales, reaching approximately ₹154.80.\nLate Afternoon: Another significant peak is observed in the late afternoon around 3 PM, with average sales of around ₹97.48.\n\nMorning vs. Afternoon Sales:\n\nMorning Hours: Generally, the morning hours witness higher sales, with an average ranging from ₹35.69 to ₹154.80 per hour.\nAfternoon Hours: After a slight dip during noon, sales pick up again, maintaining an average ranging from ₹20.49 to ₹99.16.\n\nEvening Sales Trend:\n\nEvening Hours: Sales gradually decrease in the evening hours, dropping to an average ranging from ₹9.53 to ₹29.29 during these hours.\n\nInsights:\n\nPeak Hours for Revenue: The morning hours, especially around 10 AM, contribute significantly to the daily revenue, suggesting higher customer footfall or increased spending during this time.\nAfternoon Consistency: After a slight dip during noon, sales maintain a relatively consistent pattern throughout the late afternoon.\nEvening Decline: Sales gradually decrease towards the evening, indicating a decrease in customer traffic or spending during these hours."
  },
  {
    "objectID": "pages/final.html#sales-distribution-across-time-slots-and-product-categories",
    "href": "pages/final.html#sales-distribution-across-time-slots-and-product-categories",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Sales Distribution Across Time Slots and Product Categories",
    "text": "Sales Distribution Across Time Slots and Product Categories\n\nBreakfast:\n\nTea/Coffee Preference: Dominant sales of Tea/Coffee, holding a share of approximately 29.9%, indicate a strong morning beverage preference among customers.\nQuick Bites: Maggi, Sandwich, and Vadapav collectively constitute around 43.2% of breakfast sales, indicating popularity for quick morning bites.\n\nBrunch:\n\nVaried Choices: Brunch displays a diverse range of sales across categories, with Tea/Coffee, Maggi, and Vadapav maintaining popularity, together representing about 48.6% of sales.\nContinued Preference: These items, particularly Tea/Coffee, Maggi, and Vadapav, maintain their popularity during the mid-morning period.\n\nEvening Tea Time:\n\nConsistent Choices: Tea/Coffee continues to lead during this period, representing around 26.1% of sales.\nSnacks and Accompaniments: Chaats and Sandwiches gain traction, comprising about 28.2% of sales, possibly as preferred evening accompaniments.\n\nDinner and Lunch:\n\nFocused Choices: Paratha, Chole, and Chaat dominate sales during these meals, collectively representing approximately 58.5% during dinner and around 71.9% during lunch.\nHeavier Meal Preference: Customers seem to prefer more substantial meal items during lunch and dinner times.\n\nInsights:\n\nTime-specific Preferences: Different times of the day exhibit specific consumer preferences, from morning beverages to substantial meal items.\nConsistent Favourites: Tea/Coffee remains a popular choice across multiple time slots, representing a significant portion of sales in various periods.\nVaried vs. Focused Sales: Brunch and Evening Tea Time offer a broader variety of choices, whereas Lunch and Dinner seem to focus on specific meal items, catering to more substantial meal preferences."
  },
  {
    "objectID": "pages/final.html#improved-sales-prediction-through-gradientboosting-model",
    "href": "pages/final.html#improved-sales-prediction-through-gradientboosting-model",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Improved Sales Prediction through GradientBoosting Model",
    "text": "Improved Sales Prediction through GradientBoosting Model\n\nVariable Selection and Model Building\nInitially, feature selection was performed using SelectKBest, where Feature_1 exhibited the highest score (5.35) and a statistically significant p-value (0.0208). Other features, such as Feature_2, and Feature_3, showed varied scores and p-values.\nSubsequently, five models (Model_1, Model_2, Model_3, Model_4, and Model_5) were trained with these features. Upon evaluation, Model_5 emerged with the most promising performance among these models, showcasing an R-squared value of 0.0266 and an RMSE of 207.50, indicating its superior predictive capability compared to other models.\n\n\nModel Performance Improvement\nThe previous analysis (Midterm) involved a Model_1 with an R-squared around -0.5, signifying a poor fit. However, Model_5, despite a moderately improved R-squared value of 0.0266, demonstrated a significant advancement in prediction accuracy, reducing the RMSE to 207.5. This signifies a substantial enhancement in predictive power and a departure from the poor predictive performance observed previously.\n\n\nPlot Interpretation\n\nVisualizing actual sales versus predicted sales in a line plot for total daily sales showed a notable alignment between the predicted and actual trends when employing the Model_5. This proximity between the predicted and actual trends signifies the model’s capacity to accurately capture sales patterns, offering a strong foundation for decision-making in demand forecasting and business strategies.\n\n\nInterpretation and Analysis\nDespite the moderate improvement in the R-squared value from the prior poor fit, the transition from a negative R-squared to a positive one, especially when moving from Model_1 to Model_5, is a significant achievement. The model’s demonstrated ability to closely predict actual sales trends indicates its reliability in capturing intricate sales patterns, thus providing valuable insights for informed decision-making in business strategies and demand forecasting."
  },
  {
    "objectID": "pages/final.html#rfm-analysis-overview-for-customer-segmentation",
    "href": "pages/final.html#rfm-analysis-overview-for-customer-segmentation",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "RFM Analysis Overview for Customer Segmentation",
    "text": "RFM Analysis Overview for Customer Segmentation\nThe RFM (Recency, Frequency, Monetary) analysis is a powerful technique used to segment customers based on their transaction behaviours. To derive the RFM values, the following steps were undertaken:\n\nRecency (R-score): It denotes the time since the last transaction made by each customer. Customers with lower scores have more recent purchases. Ranging from 1 (least recent) to 4 (most recent).\nFrequency (F-score): It represents the total number of transactions made by each customer during the analysed period. A higher score implies more frequent purchases. Ranging from 1 (least frequent) to 4 (most frequent).\nMonetary (M-score): It indicates the total monetary value spent by each customer. A higher score signifies higher monetary contributions. Ranging from 1 (least monetary value) to 4 (most monetary value).\n\n\nAnalysis of RFM Segmentation\n\nThe RFM_Segment column combines the individual R, F, and M scores into a three-digit number (RFM) that segments customers based on their behaviours.\nHigh-Value Customers: Customers with an RFM segment of 444 or similar (e.g., 434, 444) are high-value customers. They have made recent purchases (R-score = 4), frequent transactions (F-score = 4), and contribute significantly in terms of monetary value (M-score = 4).\nLoyal but Less Frequent Buyers: Segments like 422, 213, or 243 represent customers who might not make purchases as frequently but contribute substantially in terms of monetary value. They are recent buyers (R-score = 4) who spend well (M-score = 4 or 3) but with moderate frequency (F-score = 1 or 2).\nChurned or Dormant Customers: Customers with segments like 111 or 121 signify those who haven’t made purchases recently (R-score = 1) or spend less despite some frequency (F-score = 2 or 3).\n\nUnderstanding these segments assists the café in optimizing resource allocation. For instance, focusing on high-value customers for loyalty programs, devising strategies to increase the frequency of moderately active customers, and engaging in negotiations with the dormant ones for exclusive deals. This approach aligns with the café’s limited resources, aiming to maximize customer engagement and revenue with targeted efforts."
  },
  {
    "objectID": "pages/final.html#analysis-of-customer-segmentation-using-k-means-clustering",
    "href": "pages/final.html#analysis-of-customer-segmentation-using-k-means-clustering",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Analysis of Customer Segmentation using K-means Clustering",
    "text": "Analysis of Customer Segmentation using K-means Clustering\nThe optimal number of clusters for customer segmentation using K-means clustering was determined to be 3, employing the elbow technique. The clustering analysis was performed on the features ‘Item Price’ and ‘Quantity’.\nThe resulting clusters exhibited distinctive characteristics:\n\nCluster 1: Low Priced Items and Low Quantity Bought: This cluster comprises customers who tend to purchase low-priced items in smaller quantities.\nCluster 2: High Priced Items and Low Quantity Bought: Customers in this cluster show a preference for higher-priced items but make purchases in lower quantities.\nCluster 3: Low Priced Items and High Quantity Bought: This cluster represents customers who consistently purchase low-priced items in larger quantities.\n\nUpon analysis, an intriguing observation was made: there appeared to be no distinct cluster representing customers who purchase expensive items in high quantities. This absence suggests that within the dataset and based on the analysed features, customers who buy items in larger quantities tend to opt for lower-priced items.\n\nThe elbow plot showcasing the within-cluster sum of squares (WCSS) against the number of clusters indicated a clear ‘elbow’ or point of inflection at 3 clusters, signifying that additional clusters beyond this point do not significantly improve the model’s explanatory power.\nThe K-means clustering scatter plot, using ‘Item Price’ and ‘Quantity’ as axes, demonstrates the segregation of customers into these distinct clusters. These clusters provide valuable insights into customers’ purchasing behaviours based on price and quantity, allowing businesses to tailor marketing strategies, optimize inventory, and enhance product offerings to better meet the preferences and demands of different customer segments."
  },
  {
    "objectID": "pages/proposal.html",
    "href": "pages/proposal.html",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "",
    "text": "PDF: Proposal.pdf"
  },
  {
    "objectID": "pages/proposal.html#details-about-the-methods-used-with-justification",
    "href": "pages/proposal.html#details-about-the-methods-used-with-justification",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Details About The Methods Used With Justification",
    "text": "Details About The Methods Used With Justification\nTo address the challenges faced by XYZ Café, a two-pronged problem-solving approach is proposed. Firstly, leveraging the provided one-year inventory and sales data, a data-centric methodology will be employed to develop a robust demand estimation model for each menu item. This involves utilizing techniques such as time series analysis, regression modeling, and machine learning algorithms. By examining historical sales and inventory data, the model can provide accurate forecasts, thereby optimizing inventory management and minimizing losses.\nSecondly, to identify customer culinary preferences and demographics, data analysis will be performed to discern trends in dish sales. This entails applying segmentation and clustering methods to group customers based on their preferences. Machine learning techniques, such as k-means clustering or association rule mining, can be instrumental in this analysis. Understanding which dishes resonate with specific customer segments allows the café to tailor its offerings effectively and create targeted marketing strategies.\nThese approaches are data-driven, ensuring precision and relevance in decision-making, and are instrumental in addressing the café’s challenges. They empower XYZ Café to operate efficiently, increase profitability, and cater to its diverse customer base."
  },
  {
    "objectID": "pages/proposal.html#details-about-the-intended-data-collection-with-justification",
    "href": "pages/proposal.html#details-about-the-intended-data-collection-with-justification",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Details About the Intended Data Collection With Justification",
    "text": "Details About the Intended Data Collection With Justification\nThe primary data source for this problem-solving approach is the one-year sales and inventory data provided by the owner. This dataset encompasses crucial information, including historical sales records, menu item specifics, and inventory levels. These data points serve as the foundation for our analysis and modeling.\nTo extract meaningful insights, data cleaning and preprocessing techniques will be applied. This includes handling missing values, outliers, and ensuring data consistency. Subsequently, exploratory data analysis will be conducted to gain a comprehensive understanding of the data’s distribution and patterns.\nThe chosen analytical methods will rely exclusively on the provided dataset, as recommended by the owner, ensuring that the solutions are derived from the most relevant and practical information available. By utilizing this comprehensive dataset, we aim to develop effective demand estimation models and customer segmentation strategies, optimizing the café’s operations and enhancing its market positioning."
  },
  {
    "objectID": "pages/proposal.html#details-about-analysis-tools-with-justification",
    "href": "pages/proposal.html#details-about-analysis-tools-with-justification",
    "title": "Data-Driven Transformation: Optimizing Operations and Customer Engagement at XYZ Café",
    "section": "Details About Analysis Tools With Justification",
    "text": "Details About Analysis Tools With Justification\nThe analysis tools for this project include spreadsheets for initial data exploration and organization. Python, along with libraries like Scikit-learn for machine learning, NumPy for numerical computations, and Matplotlib for data visualization, will be employed for in-depth analysis. Python’s versatility and extensive libraries make it ideal for robust data analysis, while spreadsheets offer a user-friendly interface for initial data inspection."
  }
]